{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/total-fish-stocks.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold (30% valid data or less than 70% NaN or 0)\n",
    "threshold = 0.3 * len(data)\n",
    "\n",
    "# Filter columns based on the threshold\n",
    "data_cleaned = data.loc[:, (data != 0).sum() + data.notna().sum() - data.isna().sum() > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove duplicate or potentially misspelled columns\n",
    "data = data.rename(columns={\n",
    "    'biomass_relative_to-preferred_management_rate': 'biomass_relative_to_preferred_management_rate'\n",
    "})\n",
    "\n",
    "# Step 2: Remove \"general\" columns if they are aggregations of detailed metrics\n",
    "data = data.drop(columns=['general_total_biomass', 'general_total_catch', 'general_exploitation_rate'], errors='ignore')\n",
    "\n",
    "# Step 3: Select one baseline for \"relative to\" metrics\n",
    "columns_to_keep = [\n",
    "    'biomass_relative_to_msy', 'total_biomass_relative_to_msy', 'spawning_stock_relative_to_msy',\n",
    "    'catch_relative_to_msy', 'fishing_mortality_relative_to_msy'\n",
    "]\n",
    "relative_columns = [col for col in data.columns if '_relative_to_' in col]\n",
    "columns_to_drop = [col for col in relative_columns if col not in columns_to_keep]\n",
    "data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Step 4: Ensure columns are distinct and meaningful\n",
    "data = data.drop(columns=['total_landings'], errors='ignore')  # Assuming `total_catch` is enough\n",
    "data = data.drop(columns=['catch_relative_to_mean_catch'], errors='ignore')  # Likely less relevant\n",
    "data = data.drop(columns=['recruits'], errors='ignore')  # Highly specific, might not be necessary\n",
    "\n",
    "# Step 5: Keep only meaningful effort and biomass metrics\n",
    "data = data.drop(columns=['survey_biomass'], errors='ignore')  # If less relevant than `total_biomass`\n",
    "data = data.drop(columns=['catch_per_unit_effort'], errors='ignore')  # If `fishing_effort` suffices\n",
    "\n",
    "# Print final columns for review\n",
    "print(\"Modified Columns:\")\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Summing all numeric columns for each year\n",
    "data_grouped = data.groupby('Year').sum(numeric_only=True).reset_index()\n",
    "\n",
    "# Sort the grouped DataFrame by 'Year' in ascending order\n",
    "data_grouped = data_grouped.sort_values(by='Year', ascending=True)\n",
    "\n",
    "data_grouped.to_csv('clean1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'clean1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate the percentage of zeros in each column\n",
    "zero_percentage = (df == 0).sum() / len(df) * 100\n",
    "\n",
    "# Identify columns where zeros are less than or equal to 70%\n",
    "columns_to_keep = zero_percentage[zero_percentage <= 70].index.tolist()\n",
    "\n",
    "# Keep only these columns in the DataFrame\n",
    "df_cleaned = df[columns_to_keep]\n",
    "\n",
    "# Save or display the result\n",
    "print(\"Columns retained in the cleaned DataFrame:\")\n",
    "print(columns_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('clean2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'clean2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a function to determine if a column is discrete\n",
    "def is_discrete(column):\n",
    "    if pd.api.types.is_integer_dtype(column):\n",
    "        return True\n",
    "    if column.nunique() < 20:\n",
    "        return True\n",
    "    # Check if the column is a float but has integer-like values (e.g., 1.0, 2.0)\n",
    "    if column.dtype == 'float' and (column == column.astype(int)).all():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Iterate through the columns and fill values accordingly\n",
    "for col in df.columns:\n",
    "    if col == 'Year':  # Skip the Year column as it's temporal\n",
    "        continue\n",
    "    \n",
    "    if is_discrete(df[col]):\n",
    "        # Fill missing or zero values with the median for discrete variables\n",
    "        df[col] = df[col].replace(0, np.nan)  # Replace zeros with NaN if necessary\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        # Convert to integer if values are discrete (like 1.0, 2.0)\n",
    "        if df[col].dtype == 'float' and (df[col] == df[col].astype(int)).all():\n",
    "            df[col] = df[col].astype(int)\n",
    "        print(f'int: {col}')\n",
    "    else:\n",
    "        # Fill missing or zero values with the mean for continuous variables\n",
    "        df[col] = df[col].replace(0, np.nan)  # Replace zeros with NaN if necessary\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "        print(f'float: {col}')\n",
    "\n",
    "# Save or display the cleaned DataFrame\n",
    "print(\"Cleaned DataFrame with missing values filled:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(len(df)*0.8)\n",
    "train = df.iloc[:threshold]\n",
    "test = df.iloc[threshold:]\n",
    "\n",
    "train.to_csv('train.csv')\n",
    "test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Feature and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the train and test data\n",
    "# train_df = pd.read_csv('train.csv')\n",
    "# test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# # Check the structure of the training data\n",
    "# print(train_df.head())\n",
    "\n",
    "# # Preprocess the data\n",
    "# # Let's assume 'Year' is the time series index, and we want to predict 'total_biomass'\n",
    "# X_train = train_df.drop(columns=['total_biomass'])\n",
    "# y_train = train_df['total_biomass']\n",
    "\n",
    "# X_test = test_df.drop(columns=['total_biomass'])\n",
    "# y_test = test_df['total_biomass']\n",
    "\n",
    "# # Optionally, create lag features or other time series features\n",
    "# # For simplicity, we will use the original features, but you can engineer new ones if necessary\n",
    "\n",
    "# # Train a RandomForest model with 5-fold cross-validation\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# # 5-fold cross-validation\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Perform cross-validation and print the mean score (negative mean squared error)\n",
    "# cv_scores = cross_val_score(rf_model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "# print(f\"5-Fold Cross-Validation MSE: {-cv_scores.mean()}\")\n",
    "\n",
    "# # Train the model on the full training data\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate the Mean Squared Error (MSE) for the predictions\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Test Set Mean Squared Error: {mse}\")\n",
    "\n",
    "# # Plot the actual vs predicted values for the test set\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(test_df['Year'], y_test, label='Actual')\n",
    "# plt.plot(test_df['Year'], y_pred, label='Predicted')\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Total Biomass')\n",
    "# plt.legend()\n",
    "# plt.title('Total Biomass Prediction (Actual vs Predicted)')\n",
    "# plt.show()\n",
    "\n",
    "# # Future predictions (for example, predicting the next 5 years)\n",
    "# future_years = np.array([2017, 2018, 2019, 2020, 2021]).reshape(-1, 1)\n",
    "# # We need to create the features for these future years (e.g., using the last year data)\n",
    "# # Assuming you have an extrapolation method, here I'm using simple dummy values for future predictions\n",
    "# future_data = np.hstack([future_years, np.zeros((future_years.shape[0], X_train.shape[1] - 1))])\n",
    "\n",
    "# # Predict future biomass values\n",
    "# future_predictions = rf_model.predict(future_data)\n",
    "# print(\"Predicted Total Biomass for Future Years:\")\n",
    "# for year, prediction in zip(future_years.flatten(), future_predictions):\n",
    "#     print(f\"Year {year}: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# import lightgbm as lgb\n",
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import matplotlib.pyplot as plt\n",
    "# from datetime import datetime\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# class TimeSeriesForecaster:\n",
    "#     def __init__(self, seasonality=12):\n",
    "#         self.seasonality = seasonality\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.models = {}\n",
    "        \n",
    "#     def create_features(self, df, target_col):\n",
    "#         \"\"\"Create time series features from datetime index.\"\"\"\n",
    "#         df = df.copy()\n",
    "#         df['year'] = df['Year'].astype(int)\n",
    "        \n",
    "#         # Lag features\n",
    "#         for lag in range(1, 4):\n",
    "#             df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "            \n",
    "#         # Rolling mean features\n",
    "#         for window in [3, 6, 12]:\n",
    "#             df[f'rolling_mean_{window}'] = df[target_col].rolling(\n",
    "#                 window=window, min_periods=1).mean()\n",
    "#             df[f'rolling_std_{window}'] = df[target_col].rolling(\n",
    "#                 window=window, min_periods=1).std()\n",
    "            \n",
    "#         # Trend feature\n",
    "#         df['trend'] = np.arange(len(df))\n",
    "        \n",
    "#         return df\n",
    "    \n",
    "#     def prepare_data(self, train_df, test_df, target_col):\n",
    "#         \"\"\"Prepare data for modeling using separate train and test files.\"\"\"\n",
    "#         # Create features for both train and test\n",
    "#         train_featured = self.create_features(train_df, target_col)\n",
    "#         test_featured = self.create_features(test_df, target_col)\n",
    "        \n",
    "#         # Prepare features and target\n",
    "#         feature_cols = [col for col in train_featured.columns \n",
    "#                        if col not in [target_col, 'Year']]\n",
    "        \n",
    "#         X_train = train_featured[feature_cols].fillna(method='ffill')\n",
    "#         y_train = train_featured[target_col]\n",
    "#         X_test = test_featured[feature_cols].fillna(method='ffill')\n",
    "#         y_test = test_featured[target_col]\n",
    "        \n",
    "#         # Scale features\n",
    "#         X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "#         X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "#         return (X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "#                 train_featured, test_featured, feature_cols)\n",
    "    \n",
    "#     def train_lightgbm(self, X_train, y_train):\n",
    "#         \"\"\"Train LightGBM model with time series specific parameters.\"\"\"\n",
    "#         params = {\n",
    "#             'objective': 'regression',\n",
    "#             'metric': 'rmse',\n",
    "#             'boosting_type': 'gbdt',\n",
    "#             'num_leaves': 31,\n",
    "#             'learning_rate': 0.005,\n",
    "#             'feature_fraction': 0.9,\n",
    "#             'num_iterations': 10000,\n",
    "#             'verbose': -1\n",
    "#         }\n",
    "        \n",
    "#         # Create dataset\n",
    "#         lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        \n",
    "#         # Train model\n",
    "#         self.models['lgb'] = lgb.train(\n",
    "#             params,\n",
    "#             lgb_train,\n",
    "#             valid_sets=[lgb_train]\n",
    "#         )\n",
    "    \n",
    "#     def train_sarima(self, y_train):\n",
    "#         \"\"\"Train SARIMA model.\"\"\"\n",
    "#         self.models['sarima'] = SARIMAX(\n",
    "#             y_train,\n",
    "#             order=(1, 1, 1),\n",
    "#             seasonal_order=(1, 1, 1, self.seasonality)\n",
    "#         ).fit(disp=False)\n",
    "    \n",
    "#     def ensemble_predict(self, X_test):\n",
    "#         \"\"\"Make predictions using ensemble of models.\"\"\"\n",
    "#         # LightGBM predictions\n",
    "#         lgb_preds = self.models['lgb'].predict(X_test)\n",
    "        \n",
    "#         # SARIMA predictions\n",
    "#         sarima_preds = self.models['sarima'].forecast(len(X_test))\n",
    "        \n",
    "#         # Ensemble predictions (simple average)\n",
    "#         ensemble_preds = (lgb_preds + sarima_preds) / 2\n",
    "        \n",
    "#         return ensemble_preds\n",
    "    \n",
    "#     def plot_results(self, train_data, test_data, predictions, target_col):\n",
    "#         \"\"\"Plot actual vs predicted values.\"\"\"\n",
    "#         plt.figure(figsize=(12, 6))\n",
    "#         plt.plot(train_data['Year'], train_data[target_col], \n",
    "#                 label='Training Data', color='blue')\n",
    "#         plt.plot(test_data['Year'], test_data[target_col], \n",
    "#                 label='Actual Test Data', color='green')\n",
    "#         plt.plot(test_data['Year'], predictions, \n",
    "#                 label='Predictions', color='red', linestyle='--')\n",
    "#         plt.title('Time Series Forecasting Results')\n",
    "#         plt.xlabel('Year')\n",
    "#         plt.ylabel(target_col)\n",
    "#         plt.legend()\n",
    "#         plt.grid(True)\n",
    "#         plt.show()\n",
    "    \n",
    "#     def fit_predict(self, train_df, test_df, target_col):\n",
    "#         \"\"\"Full training and prediction pipeline.\"\"\"\n",
    "#         # Prepare data\n",
    "#         (X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "#          train_data, test_data, feature_cols) = self.prepare_data(\n",
    "#             train_df, test_df, target_col)\n",
    "        \n",
    "#         # Train models\n",
    "#         self.train_lightgbm(X_train_scaled, y_train)\n",
    "#         self.train_sarima(y_train)\n",
    "        \n",
    "#         # Make predictions\n",
    "#         predictions = self.ensemble_predict(X_test_scaled)\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         mse = mean_squared_error(y_test, predictions)\n",
    "#         r2 = r2_score(y_test, predictions)\n",
    "        \n",
    "#         print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "#         print(f\"R² Score: {r2:.2f}\")\n",
    "        \n",
    "#         # Plot results\n",
    "#         self.plot_results(train_data, test_data, predictions, target_col)\n",
    "        \n",
    "#         return predictions\n",
    "    \n",
    "#     def forecast_future(self, train_df, periods=5):\n",
    "#         \"\"\"Forecast future values.\"\"\"\n",
    "#         last_data = train_df.iloc[-1:]\n",
    "#         future_dates = pd.DataFrame({\n",
    "#             'Year': range(\n",
    "#                 int(last_data['Year'].values[0]) + 1,\n",
    "#                 int(last_data['Year'].values[0]) + periods + 1\n",
    "#             )\n",
    "#         })\n",
    "        \n",
    "#         # Add target column to future dates for feature creation\n",
    "#         future_dates['total_biomass'] = np.nan\n",
    "        \n",
    "#         # Combine historical and future data\n",
    "#         full_data = pd.concat([train_df, future_dates], ignore_index=True)\n",
    "        \n",
    "#         # Create features for all data\n",
    "#         featured_data = self.create_features(full_data, 'total_biomass')\n",
    "        \n",
    "#         # Get predictions for future dates\n",
    "#         future_features = featured_data.iloc[-periods:].copy()\n",
    "#         feature_cols = [col for col in featured_data.columns \n",
    "#                        if col not in ['total_biomass', 'Year']]\n",
    "        \n",
    "#         X_future = future_features[feature_cols].fillna(method='ffill')\n",
    "#         X_future_scaled = self.scaler.transform(X_future)\n",
    "        \n",
    "#         predictions = self.ensemble_predict(X_future_scaled)\n",
    "        \n",
    "#         return future_dates['Year'], predictions\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load data\n",
    "#     train_df = pd.read_csv('train.csv')\n",
    "#     test_df = pd.read_csv('test.csv')\n",
    "    \n",
    "#     # Initialize and train the model\n",
    "#     forecaster = TimeSeriesForecaster(seasonality=12)\n",
    "#     predictions = forecaster.fit_predict(train_df, test_df, 'total_biomass')\n",
    "    \n",
    "#     # Make future predictions\n",
    "#     future_years, future_predictions = forecaster.forecast_future(train_df, periods=5)\n",
    "#     print(\"\\nFuture Predictions:\")\n",
    "#     for year, pred in zip(future_years, future_predictions):\n",
    "#         print(f\"Year {year}: {pred:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TimeSeriesForecaster:\n",
    "    def __init__(self, seasonality=12):\n",
    "        self.seasonality = seasonality\n",
    "        self.scaler = RobustScaler()\n",
    "        self.models = {}\n",
    "\n",
    "    def plot_results(self, train_data, test_data, predictions, target_col):\n",
    "        \"\"\"Plot actual vs predicted values.\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot training data\n",
    "        plt.plot(train_data['Year'], train_data[target_col], \n",
    "                label='Training Data', color='blue', alpha=0.7)\n",
    "        \n",
    "        # Plot test data\n",
    "        plt.plot(test_data['Year'], test_data[target_col], \n",
    "                label='Actual Test Data', color='green', alpha=0.7)\n",
    "        \n",
    "        # Plot predictions\n",
    "        plt.plot(test_data['Year'], predictions, \n",
    "                label='Predictions', color='red', \n",
    "                linestyle='--', alpha=0.8)\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Time Series Forecasting Results', fontsize=14, pad=20)\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel(target_col, fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Adjust layout to prevent label cutoff\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def create_features(self, df, target_col):\n",
    "        \"\"\"Create enhanced time series features from datetime index.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['year'] = df['Year'].astype(int)\n",
    "        \n",
    "        # Enhanced lag features\n",
    "        for lag in range(1, 7):\n",
    "            df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "        \n",
    "        # Enhanced rolling statistics\n",
    "        for window in [3, 6, 12, 24]:\n",
    "            df[f'rolling_mean_{window}'] = df[target_col].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "            df[f'rolling_std_{window}'] = df[target_col].rolling(\n",
    "                window=window, min_periods=1).std()\n",
    "            df[f'rolling_min_{window}'] = df[target_col].rolling(\n",
    "                window=window, min_periods=1).min()\n",
    "            df[f'rolling_max_{window}'] = df[target_col].rolling(\n",
    "                window=window, min_periods=1).max()\n",
    "            \n",
    "        # Exponential moving averages\n",
    "        for span in [3, 6, 12]:\n",
    "            df[f'ema_{span}'] = df[target_col].ewm(span=span, adjust=False).mean()\n",
    "        \n",
    "        # Trend and cyclical features\n",
    "        df['trend'] = np.arange(len(df))\n",
    "        df['trend_squared'] = df['trend'] ** 2\n",
    "        df['year_mod'] = df['year'] % self.seasonality\n",
    "        \n",
    "        # Difference features\n",
    "        df['diff_1'] = df[target_col].diff()\n",
    "        df['diff_2'] = df[target_col].diff().diff()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, train_df, test_df, target_col):\n",
    "        \"\"\"Prepare data with enhanced preprocessing.\"\"\"\n",
    "        # Create features\n",
    "        train_featured = self.create_features(train_df, target_col)\n",
    "        test_featured = self.create_features(test_df, target_col)\n",
    "        \n",
    "        # Prepare features and target\n",
    "        feature_cols = [col for col in train_featured.columns \n",
    "                       if col not in [target_col, 'Year']]\n",
    "        \n",
    "        X_train = train_featured[feature_cols].copy()\n",
    "        y_train = train_featured[target_col]\n",
    "        X_test = test_featured[feature_cols].copy()\n",
    "        y_test = test_featured[target_col]\n",
    "        \n",
    "        # Handle missing values more robustly\n",
    "        for col in X_train.columns:\n",
    "            X_train[col] = X_train[col].fillna(X_train[col].median())\n",
    "            X_test[col] = X_test[col].fillna(X_test[col].median())\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        return (X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "                train_featured, test_featured, feature_cols)\n",
    "    \n",
    "    def train_lightgbm(self, X_train, y_train):\n",
    "        \"\"\"Train LightGBM with optimized parameters.\"\"\"\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 20,\n",
    "            'learning_rate': 0.0001,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'num_iterations': 5000,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'min_child_samples': 20\n",
    "        }\n",
    "        \n",
    "        # Create dataset with validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_train_split, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_train_split, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "            \n",
    "        lgb_train = lgb.Dataset(X_train_split, y_train_split)\n",
    "        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "        \n",
    "        # Train model\n",
    "        self.models['lgb'] = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_train, lgb_val],\n",
    "            valid_names=['train', 'valid']\n",
    "        )\n",
    "    \n",
    "    def train_sarima(self, y_train):\n",
    "        \"\"\"Train SARIMA with optimized parameters.\"\"\"\n",
    "        # Grid search for best parameters\n",
    "        best_aic = float('inf')\n",
    "        best_order = None\n",
    "        best_seasonal_order = None\n",
    "        \n",
    "        for p in range(2):\n",
    "            for d in range(2):\n",
    "                for q in range(2):\n",
    "                    for P in range(2):\n",
    "                        for D in range(2):\n",
    "                            for Q in range(2):\n",
    "                                try:\n",
    "                                    model = SARIMAX(\n",
    "                                        y_train,\n",
    "                                        order=(p, d, q),\n",
    "                                        seasonal_order=(P, D, Q, self.seasonality)\n",
    "                                    ).fit(disp=False)\n",
    "                                    if model.aic < best_aic:\n",
    "                                        best_aic = model.aic\n",
    "                                        best_order = (p, d, q)\n",
    "                                        best_seasonal_order = (P, D, Q, self.seasonality)\n",
    "                                except:\n",
    "                                    continue\n",
    "        \n",
    "        # Train final model with best parameters\n",
    "        self.models['sarima'] = SARIMAX(\n",
    "            y_train,\n",
    "            order=best_order if best_order else (1, 1, 1),\n",
    "            seasonal_order=best_seasonal_order if best_seasonal_order else (1, 1, 1, self.seasonality)\n",
    "        ).fit(disp=False)\n",
    "    \n",
    "    def ensemble_predict(self, X_test):\n",
    "        \"\"\"Make weighted ensemble predictions.\"\"\"\n",
    "        # Get predictions from both models\n",
    "        lgb_preds = self.models['lgb'].predict(X_test)\n",
    "        sarima_preds = self.models['sarima'].forecast(len(X_test))\n",
    "        \n",
    "        # Use weighted average (giving more weight to LightGBM as it handles features better)\n",
    "        ensemble_preds = 0.7 * lgb_preds + 0.3 * sarima_preds\n",
    "        \n",
    "        return ensemble_preds\n",
    "\n",
    "    def fit_predict(self, train_df, test_df, target_col):\n",
    "        \"\"\"Full training and prediction pipeline with error handling.\"\"\"\n",
    "        try:\n",
    "            # Prepare data\n",
    "            (X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "             train_data, test_data, feature_cols) = self.prepare_data(\n",
    "                train_df, test_df, target_col)\n",
    "            \n",
    "            # Train models\n",
    "            self.train_lightgbm(X_train_scaled, y_train)\n",
    "            self.train_sarima(y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.ensemble_predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            \n",
    "            print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "            print(f\"R² Score: {r2:.2f}\")\n",
    "            \n",
    "            # Plot results\n",
    "            self.plot_results(train_data, test_data, predictions, target_col)\n",
    "            \n",
    "            return predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fit_predict: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Initialize and train the model\n",
    "forecaster = TimeSeriesForecaster(seasonality=12)\n",
    "predictions = forecaster.fit_predict(train_df, test_df, 'total_biomass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Load your data\n",
    "df = pd.read_csv('clean3.csv')\n",
    "\n",
    "# Drop 'Year' column\n",
    "df_without_year = df.drop(columns=['Year'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_without_year.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix Excluding Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TimeSeriesForecaster:\n",
    "    def __init__(self, seasonality=12):\n",
    "        self.seasonality = seasonality\n",
    "        self.robust_scaler = RobustScaler()\n",
    "        self.lstm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.models = {}\n",
    "\n",
    "    def plot_results(self, train_data, test_data, predictions, target_col):\n",
    "        \"\"\"Plot actual vs predicted values.\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(train_data['Year'], train_data[target_col], label='Training Data', color='blue', alpha=0.7)\n",
    "        plt.plot(test_data['Year'], test_data[target_col], label='Actual Test Data', color='green', alpha=0.7)\n",
    "        plt.plot(test_data['Year'], predictions, label='Predictions', color='red', linestyle='--', alpha=0.8)\n",
    "        plt.title('Time Series Forecasting Results', fontsize=14, pad=20)\n",
    "        plt.xlabel('Year', fontsize=12)\n",
    "        plt.ylabel(target_col, fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def create_features(self, df, target_col):\n",
    "        \"\"\"Create enhanced time series features.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['year'] = df['Year'].astype(int)\n",
    "        for lag in range(1, 7):\n",
    "            df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "        for window in [3, 6, 12]:\n",
    "            df[f'rolling_mean_{window}'] = df[target_col].rolling(window=window, min_periods=1).mean()\n",
    "            df[f'rolling_std_{window}'] = df[target_col].rolling(window=window, min_periods=1).std()\n",
    "        df['trend'] = np.arange(len(df))\n",
    "        df['year_mod'] = df['year'] % self.seasonality\n",
    "        df['diff_1'] = df[target_col].diff()\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, train_df, test_df, target_col):\n",
    "        \"\"\"Prepare data for training and testing.\"\"\"\n",
    "        train_featured = self.create_features(train_df, target_col)\n",
    "        test_featured = self.create_features(test_df, target_col)\n",
    "        feature_cols = [col for col in train_featured.columns if col not in [target_col, 'Year']]\n",
    "        X_train = train_featured[feature_cols].fillna(train_featured[feature_cols].median())\n",
    "        y_train = train_featured[target_col]\n",
    "        X_test = test_featured[feature_cols].fillna(test_featured[feature_cols].median())\n",
    "        y_test = test_featured[target_col]\n",
    "        X_train_scaled = self.robust_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.robust_scaler.transform(X_test)\n",
    "        return X_train_scaled, y_train, X_test_scaled, y_test, train_featured, test_featured, feature_cols\n",
    "\n",
    "    def prepare_lstm_data(self, X, y, time_steps=3):\n",
    "        \"\"\"Prepare data for LSTM.\"\"\"\n",
    "        X_lstm, y_lstm = [], []\n",
    "        for i in range(time_steps, len(X)):\n",
    "            X_lstm.append(X[i-time_steps:i, :])\n",
    "            y_lstm.append(y[i])\n",
    "        return np.array(X_lstm), np.array(y_lstm)\n",
    "\n",
    "    def train_lightgbm(self, X_train, y_train):\n",
    "        \"\"\"Train LightGBM model.\"\"\"\n",
    "        params = {\n",
    "            'objective': 'regression', 'metric': 'rmse', 'boosting_type': 'gbdt',\n",
    "            'num_leaves': 20, 'learning_rate': 0.01, 'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8, 'bagging_freq': 5, 'num_iterations': 5000,\n",
    "            'early_stopping_rounds': 100, 'verbose': -1\n",
    "        }\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_train_split, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_train_split, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        lgb_train = lgb.Dataset(X_train_split, y_train_split)\n",
    "        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "        self.models['lgb'] = lgb.train(\n",
    "            params, lgb_train, valid_sets=[lgb_train, lgb_val], valid_names=['train', 'valid']\n",
    "        )\n",
    "\n",
    "    def train_sarima(self, y_train):\n",
    "        \"\"\"Train SARIMA model.\"\"\"\n",
    "        best_aic = float('inf')\n",
    "        best_order = None\n",
    "        best_seasonal_order = None\n",
    "        for p in range(2):\n",
    "            for d in range(2):\n",
    "                for q in range(2):\n",
    "                    for P in range(2):\n",
    "                        for D in range(2):\n",
    "                            for Q in range(2):\n",
    "                                try:\n",
    "                                    model = SARIMAX(\n",
    "                                        y_train, order=(p, d, q),\n",
    "                                        seasonal_order=(P, D, Q, self.seasonality)\n",
    "                                    ).fit(disp=False)\n",
    "                                    if model.aic < best_aic:\n",
    "                                        best_aic = model.aic\n",
    "                                        best_order = (p, d, q)\n",
    "                                        best_seasonal_order = (P, D, Q, self.seasonality)\n",
    "                                except:\n",
    "                                    continue\n",
    "        self.models['sarima'] = SARIMAX(\n",
    "            y_train, order=best_order, seasonal_order=best_seasonal_order\n",
    "        ).fit(disp=False)\n",
    "\n",
    "    def train_lstm(self, X_train, y_train, time_steps=3):\n",
    "        \"\"\"Train LSTM model.\"\"\"\n",
    "        X_train_lstm, y_train_lstm = self.prepare_lstm_data(X_train, y_train, time_steps)\n",
    "        model = Sequential([\n",
    "            LSTM(20, return_sequences=False, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "            Dropout(0.2), Dense(10, activation='relu'), Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "        model.fit(X_train_lstm, y_train_lstm, epochs=50, batch_size=32, verbose=1, callbacks=[early_stopping])\n",
    "        self.models['lstm'] = model\n",
    "\n",
    "    def predict_lstm(self, X_test, time_steps=3):\n",
    "        \"\"\"Make predictions with LSTM.\"\"\"\n",
    "        X_test_lstm, _ = self.prepare_lstm_data(X_test, np.zeros(len(X_test)))\n",
    "        return self.models['lstm'].predict(X_test_lstm).flatten()\n",
    "\n",
    "    def fit_predict(self, train_df, test_df, target_col):\n",
    "        \"\"\"Complete pipeline for training and prediction.\"\"\"\n",
    "        try:\n",
    "            X_train_scaled, y_train, X_test_scaled, y_test, train_data, test_data, _ = \\\n",
    "                self.prepare_data(train_df, test_df, target_col)\n",
    "            X_train_lstm = self.lstm_scaler.fit_transform(X_train_scaled)\n",
    "            y_train_lstm = self.lstm_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "            X_test_lstm = self.lstm_scaler.transform(X_test_scaled)\n",
    "            self.train_lightgbm(X_train_scaled, y_train)\n",
    "            self.train_sarima(y_train)\n",
    "            self.train_lstm(X_train_lstm, y_train_lstm)\n",
    "            lgb_preds = self.models['lgb'].predict(X_test_scaled)\n",
    "            sarima_preds = self.models['sarima'].forecast(len(y_test))\n",
    "            lstm_preds = self.predict_lstm(X_test_lstm)\n",
    "            predictions = 0.5 * lgb_preds + 0.3 * sarima_preds + 0.2 * lstm_preds\n",
    "            mse = mean_squared_error(y_test, predictions)\n",
    "            r2 = r2_score(y_test, predictions)\n",
    "            print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "            print(f\"R² Score: {r2:.2f}\")\n",
    "            self.plot_results(train_data, test_data, predictions, target_col)\n",
    "            return predictions\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fit_predict: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "forecaster = TimeSeriesForecaster(seasonality=12)\n",
    "predictions = forecaster.fit_predict(train_df, test_df, target_col='target_column')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
